#!/usr/bin/env python3
"""
ç­–ç•¥è¿è¡Œç»Ÿè®¡ä»ªè¡¨æ¿
æ˜¾ç¤ºå®æ—¶ç»Ÿè®¡æ•°æ®å’Œæ¨¡å‹æ€§èƒ½
"""
from __future__ import annotations

import sys
from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))


def print_header(title):
    """æ‰“å°æ ‡é¢˜"""
    print("\n" + "=" * 80)
    print(f"  {title}")
    print("=" * 80)


def show_signal_statistics():
    """æ˜¾ç¤ºä¿¡å·ç»Ÿè®¡"""
    print_header("ğŸ“Š ä¿¡å·ç»Ÿè®¡")

    signals_file = ROOT / "data" / "paper_trading" / "signals_history.csv"

    if not signals_file.exists():
        print("âŒ æ— ä¿¡å·å†å²æ•°æ®")
        return

    signals = pd.read_csv(signals_file)
    signals['signal_time'] = pd.to_datetime(signals['signal_time'])
    signals['signal_date'] = signals['signal_time'].dt.date

    # æ€»ä½“ç»Ÿè®¡
    print(f"\nğŸ“ˆ æ€»ä½“æ•°æ®:")
    print(f"  æ€»ä¿¡å·æ•°: {len(signals)}")
    print(f"  ç‹¬ç«‹æ ‡çš„: {signals['symbol'].nunique()}")
    print(f"  æ—¥æœŸèŒƒå›´: {signals['signal_date'].min()} ~ {signals['signal_date'].max()}")

    # æœ€è¿‘ç»Ÿè®¡
    last_7_days = datetime.now().date() - timedelta(days=7)
    recent = signals[signals['signal_date'] >= last_7_days]

    print(f"\nğŸ“… æœ€è¿‘7å¤©:")
    print(f"  ä¿¡å·æ•°: {len(recent)}")
    print(f"  æ—¥å‡: {len(recent) / 7:.1f}")

    # æ¨¡å‹åˆ†ç±»
    if 'model_class' in signals.columns:
        print(f"\nğŸ¤– æ¨¡å‹åˆ†ç±»åˆ†å¸ƒ:")
        class_counts = signals['model_class'].value_counts()
        for cls, count in class_counts.items():
            pct = count / len(signals) * 100
            print(f"  Class {cls}: {count} ({pct:.1f}%)")

    # æ¯æ—¥ç»Ÿè®¡
    print(f"\nğŸ“† æœ€è¿‘5å¤©æ˜ç»†:")
    last_5_days = datetime.now().date() - timedelta(days=5)
    recent_5d = signals[signals['signal_date'] >= last_5_days]

    daily = recent_5d.groupby('signal_date').agg({
        'symbol': 'count',
        'model_score': 'mean' if 'model_score' in signals.columns else 'first'
    }).rename(columns={'symbol': 'ä¿¡å·æ•°'})

    if 'model_score' in signals.columns:
        daily = daily.rename(columns={'model_score': 'å¹³å‡åˆ†'})
        print(daily.to_string())
    else:
        print(daily[['ä¿¡å·æ•°']].to_string())


def show_training_data():
    """æ˜¾ç¤ºè®­ç»ƒæ•°æ®ç»Ÿè®¡"""
    print_header("ğŸ“š è®­ç»ƒæ•°æ®")

    backtest_file = ROOT / "data" / "backtest_trades.csv"

    if not backtest_file.exists():
        print("âŒ æ— å›æµ‹äº¤æ˜“æ•°æ®")
        return

    trades = pd.read_csv(backtest_file)

    print(f"\nğŸ’¹ äº¤æ˜“è®°å½•:")
    print(f"  æ€»äº¤æ˜“æ•°: {len(trades)}")

    if 'pnl_pct' in trades.columns:
        print(f"  å¹³å‡æ”¶ç›Š: {trades['pnl_pct'].mean():.2%}")
        print(f"  ä¸­ä½æ”¶ç›Š: {trades['pnl_pct'].median():.2%}")
        print(f"  èƒœç‡: {(trades['pnl_pct'] > 0).sum() / len(trades):.1%}")

        # æ”¶ç›Šåˆ†å¸ƒ
        print(f"\nğŸ“Š æ”¶ç›Šåˆ†å¸ƒ:")
        print(f"  >10%: {(trades['pnl_pct'] > 0.10).sum()} ({(trades['pnl_pct'] > 0.10).sum() / len(trades) * 100:.1f}%)")
        print(f"  5%-10%: {((trades['pnl_pct'] > 0.05) & (trades['pnl_pct'] <= 0.10)).sum()}")
        print(f"  0%-5%: {((trades['pnl_pct'] > 0) & (trades['pnl_pct'] <= 0.05)).sum()}")
        print(f"  äºæŸ: {(trades['pnl_pct'] < 0).sum()} ({(trades['pnl_pct'] < 0).sum() / len(trades) * 100:.1f}%)")

    # æŒ‰æ¨¡å‹åˆ†ç±»
    if 'model_class' in trades.columns:
        print(f"\nğŸ¯ æŒ‰æ¨¡å‹åˆ†ç±»è¡¨ç°:")
        for cls in sorted(trades['model_class'].unique()):
            cls_trades = trades[trades['model_class'] == cls]
            if 'pnl_pct' in trades.columns:
                print(f"  Class {cls}: {len(cls_trades)} ç¬”, "
                      f"å¹³å‡æ”¶ç›Š {cls_trades['pnl_pct'].mean():.2%}, "
                      f"èƒœç‡ {(cls_trades['pnl_pct'] > 0).sum() / len(cls_trades):.1%}")


def show_model_info():
    """æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯"""
    print_header("ğŸ¤– æ¨¡å‹ä¿¡æ¯")

    model_file = ROOT / "models" / "rank_model.pt"
    model_meta_file = ROOT / "models" / "rank_model_meta.json"

    if not model_file.exists():
        print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        return

    # æ¨¡å‹æ–‡ä»¶ä¿¡æ¯
    model_mtime = datetime.fromtimestamp(model_file.stat().st_mtime)
    model_size = model_file.stat().st_size / 1024 / 1024

    print(f"\nğŸ“¦ æ¨¡å‹æ–‡ä»¶:")
    print(f"  è·¯å¾„: {model_file}")
    print(f"  å¤§å°: {model_size:.2f} MB")
    print(f"  è®­ç»ƒæ—¶é—´: {model_mtime.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"  è·ä»Š: {(datetime.now() - model_mtime).total_seconds() / 3600:.1f} å°æ—¶")

    # æ¨¡å‹å…ƒæ•°æ®
    if model_meta_file.exists():
        import json
        with open(model_meta_file, 'r') as f:
            meta = json.load(f)

        print(f"\nâš™ï¸  æ¨¡å‹é…ç½®:")
        print(f"  è®­ç»ƒæ ·æœ¬æ•°: {meta.get('train_samples', 'N/A')}")
        print(f"  æµ‹è¯•æ ·æœ¬æ•°: {meta.get('test_samples', 'N/A')}")
        print(f"  æµ‹è¯•å‡†ç¡®ç‡: {meta.get('test_accuracy', 0) * 100:.1f}%")
        print(f"  è®­ç»ƒè½®æ•°: {meta.get('epochs', 'N/A')}")
        print(f"  åºåˆ—é•¿åº¦: {meta.get('seq_len', 'N/A')}")


def show_hourly_stats():
    """æ˜¾ç¤ºæ¯å°æ—¶ç»Ÿè®¡"""
    print_header("â° æ¯å°æ—¶è¿è¡Œç»Ÿè®¡")

    stats_dir = ROOT / "data" / "statistics"
    current_month = datetime.now().strftime('%Y%m')
    stats_file = stats_dir / f"stats_{current_month}.csv"

    if not stats_file.exists():
        print("âŒ æ— ç»Ÿè®¡æ•°æ®")
        return

    stats = pd.read_csv(stats_file)
    stats['timestamp'] = pd.to_datetime(stats['timestamp'])

    print(f"\nğŸ“Š æœ¬æœˆç»Ÿè®¡ ({current_month}):")
    print(f"  è¿è¡Œæ¬¡æ•°: {len(stats)}")
    print(f"  æœ€è¿‘è¿è¡Œ: {stats['timestamp'].max().strftime('%Y-%m-%d %H:%M:%S')}")

    # æœ€è¿‘24å°æ—¶
    last_24h = datetime.now() - timedelta(hours=24)
    recent = stats[stats['timestamp'] >= last_24h]

    if len(recent) > 0:
        print(f"\nâ±ï¸  æœ€è¿‘24å°æ—¶:")
        print(f"  è¿è¡Œæ¬¡æ•°: {len(recent)}")
        print(f"  å¹³å‡ä¿¡å·æ•°: {recent['recent_7d_count'].mean():.1f}")

    # æ˜¾ç¤ºæœ€è¿‘5æ¬¡è¿è¡Œ
    print(f"\nğŸ• æœ€è¿‘5æ¬¡è¿è¡Œ:")
    recent_5 = stats.tail(5)[['timestamp', 'total_signals', 'unique_symbols', 'recent_7d_count']]
    recent_5['timestamp'] = recent_5['timestamp'].dt.strftime('%m-%d %H:%M')
    recent_5 = recent_5.rename(columns={
        'timestamp': 'æ—¶é—´',
        'total_signals': 'æ€»ä¿¡å·',
        'unique_symbols': 'æ ‡çš„æ•°',
        'recent_7d_count': 'è¿‘7å¤©'
    })
    print(recent_5.to_string(index=False))


def show_coin_pool():
    """æ˜¾ç¤ºå¸æ± ä¿¡æ¯"""
    print_header("ğŸ’° å¸æ± ä¿¡æ¯")

    daily_dir = ROOT / "data" / "daily_klines"
    hourly_dir = ROOT / "data" / "hourly_klines"

    daily_count = len(list(daily_dir.glob("*.csv")))
    hourly_count = len(list(hourly_dir.glob("*.csv")))

    print(f"\nğŸ“ æ•°æ®æ–‡ä»¶:")
    print(f"  æ—¥çº¿æ–‡ä»¶: {daily_count} ä¸ª")
    print(f"  å°æ—¶çº¿æ–‡ä»¶: {hourly_count} ä¸ª")

    # éšæœºæ˜¾ç¤ºå‡ ä¸ªå¸ç§
    if daily_count > 0:
        files = list(daily_dir.glob("*.csv"))[:10]
        print(f"\nğŸª™ ç¤ºä¾‹å¸ç§ï¼ˆå‰10ä¸ªï¼‰:")
        for f in files:
            symbol = f.stem.replace('_1d', '')
            print(f"  - {symbol}")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "ğŸ¯" * 40)
    print("  Quant4Little ç­–ç•¥ç›‘æ§ä»ªè¡¨æ¿")
    print("  æ›´æ–°æ—¶é—´: " + datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    print("ğŸ¯" * 40)

    show_signal_statistics()
    show_training_data()
    show_model_info()
    show_hourly_stats()
    show_coin_pool()

    print("\n" + "=" * 80)
    print("  ä»ªè¡¨æ¿æ˜¾ç¤ºå®Œæˆ")
    print("=" * 80 + "\n")


if __name__ == "__main__":
    main()
