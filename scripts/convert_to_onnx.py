#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PyTorchæ¨¡å‹è½¬ONNX - ç”¨äºECSä½å†…å­˜éƒ¨ç½²

å°†PyTorch Transformeræ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼
å†…å­˜å ç”¨: 800MB â†’ 400MB
"""
from __future__ import annotations

import io
import sys
from pathlib import Path
import json

# ä¿®å¤Windowsæ§åˆ¶å°ç¼–ç é—®é¢˜
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

try:
    import torch
    import torch.onnx
except ImportError:
    print("âŒ è¯·å…ˆå®‰è£…PyTorch: pip install torch")
    sys.exit(1)

try:
    import onnx
    import onnxruntime as ort
except ImportError:
    print("âŒ è¯·å…ˆå®‰è£…ONNX: pip install onnx onnxruntime")
    sys.exit(1)

from scripts.modeling.train_ranker import TransformerRanker


def convert_to_onnx(
    pytorch_model_path: Path = Path("models/rank_model.pt"),
    onnx_model_path: Path = Path("models/rank_model.onnx"),
    opset_version: int = 14
):
    """
    è½¬æ¢PyTorchæ¨¡å‹ä¸ºONNXæ ¼å¼

    Args:
        pytorch_model_path: PyTorchæ¨¡å‹è·¯å¾„
        onnx_model_path: è¾“å‡ºONNXæ¨¡å‹è·¯å¾„
        opset_version: ONNX opsetç‰ˆæœ¬
    """
    print(f"\n{'='*70}")
    print(f"  ğŸ”„ PyTorch â†’ ONNX æ¨¡å‹è½¬æ¢")
    print(f"{'='*70}\n")

    # 1. åŠ è½½å…ƒæ•°æ®
    print(f"[1/5] åŠ è½½æ¨¡å‹å’Œå…ƒæ•°æ®")

    if not pytorch_model_path.exists():
        print(f"  âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {pytorch_model_path}")
        return False

    meta_path = pytorch_model_path.parent / "rank_model_meta.json"
    if not meta_path.exists():
        print(f"  âŒ å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {meta_path}")
        return False

    try:
        # åŠ è½½å…ƒæ•°æ®
        with open(meta_path, 'r') as f:
            meta = json.load(f)

        # åŠ è½½æ¨¡å‹state_dict
        state_dict = torch.load(pytorch_model_path, map_location='cpu')

        # ä»å…ƒæ•°æ®ä¸­æå–æ¨¡å‹é…ç½®
        seq_len = meta['seq_len']
        seq_dim = meta['model_kwargs']['seq_dim']
        tab_dim = meta['model_kwargs']['feature_dim']
        num_classes = len(meta['class_values'])

        print(f"  âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"  é…ç½®: tab_dim={tab_dim}, seq_len={seq_len}, "
              f"seq_dim={seq_dim}, num_classes={num_classes}")

    except Exception as e:
        print(f"  âŒ åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    # 2. é‡å»ºæ¨¡å‹
    print(f"\n[2/5] é‡å»ºæ¨¡å‹ç»“æ„")

    try:
        model = TransformerRanker(
            feature_dim=tab_dim,  # æ³¨æ„ï¼šå‚æ•°åæ˜¯feature_dimä¸æ˜¯tab_dim
            seq_len=seq_len,
            seq_dim=seq_dim,
            num_classes=num_classes,
            d_model=64,  # ä»é”™è¯¯ä¿¡æ¯çœ‹å‡ºæ˜¯64ä¸æ˜¯128
            nhead=4,
            num_layers=2,
            dropout=0.1
        )

        model.load_state_dict(state_dict)
        model.eval()

        print(f"  âœ… æ¨¡å‹ç»“æ„é‡å»ºæˆåŠŸ")

    except Exception as e:
        print(f"  âŒ é‡å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    # 3. åˆ›å»ºè™šæ‹Ÿè¾“å…¥
    print(f"\n[3/5] åˆ›å»ºè™šæ‹Ÿè¾“å…¥")

    dummy_tabular = torch.randn(1, tab_dim)
    dummy_sequence = torch.randn(1, seq_len, seq_dim)

    print(f"  Tabular: {dummy_tabular.shape}")
    print(f"  Sequence: {dummy_sequence.shape}")

    # 4. å¯¼å‡ºONNX
    print(f"\n[4/5] å¯¼å‡ºONNXæ¨¡å‹")

    try:
        torch.onnx.export(
            model,
            (dummy_tabular, dummy_sequence),
            str(onnx_model_path),
            export_params=True,
            opset_version=opset_version,
            do_constant_folding=True,
            input_names=['tabular', 'sequence'],
            output_names=['output'],
            dynamic_axes={
                'tabular': {0: 'batch_size'},
                'sequence': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            }
        )

        print(f"  âœ… ONNXæ¨¡å‹å¯¼å‡ºæˆåŠŸ: {onnx_model_path}")

    except Exception as e:
        print(f"  âŒ å¯¼å‡ºå¤±è´¥: {e}")
        return False

    # 5. éªŒè¯ONNXæ¨¡å‹
    print(f"\n[5/5] éªŒè¯ONNXæ¨¡å‹")

    try:
        # æ£€æŸ¥æ¨¡å‹
        onnx_model = onnx.load(str(onnx_model_path))
        onnx.checker.check_model(onnx_model)
        print(f"  âœ… ONNXæ¨¡å‹ç»“æ„éªŒè¯é€šè¿‡")

        # æµ‹è¯•æ¨ç†
        ort_session = ort.InferenceSession(
            str(onnx_model_path),
            providers=['CPUExecutionProvider']
        )

        ort_inputs = {
            'tabular': dummy_tabular.numpy(),
            'sequence': dummy_sequence.numpy()
        }

        ort_outputs = ort_session.run(None, ort_inputs)
        pytorch_outputs = model(dummy_tabular, dummy_sequence).detach().numpy()

        # å¯¹æ¯”è¾“å‡º
        import numpy as np
        diff = np.abs(ort_outputs[0] - pytorch_outputs).max()

        print(f"  PyTorch vs ONNX æœ€å¤§è¯¯å·®: {diff:.6f}")

        if diff < 1e-4:
            print(f"  âœ… æ¨ç†ç»“æœä¸€è‡´ï¼ˆè¯¯å·® < 1e-4ï¼‰")
        else:
            print(f"  âš ï¸  æ¨ç†ç»“æœå­˜åœ¨å·®å¼‚ï¼ˆè¯¯å·® {diff:.6f}ï¼‰")

    except Exception as e:
        print(f"  âŒ éªŒè¯å¤±è´¥: {e}")
        return False

    # 6. ä¿å­˜å…ƒæ•°æ®
    meta_path = onnx_model_path.parent / "rank_model_meta.json"
    with open(meta_path, 'w') as f:
        json.dump(meta, f, indent=2)

    print(f"  âœ… å…ƒæ•°æ®å·²ä¿å­˜: {meta_path}")

    # 7. æ–‡ä»¶å¤§å°å¯¹æ¯”
    print(f"\n{'='*70}")
    print(f"  ğŸ“Š æ–‡ä»¶å¤§å°å¯¹æ¯”")
    print(f"{'='*70}")

    pytorch_size = pytorch_model_path.stat().st_size / 1024 / 1024
    onnx_size = onnx_model_path.stat().st_size / 1024 / 1024

    print(f"  PyTorch: {pytorch_size:.2f} MB")
    print(f"  ONNX:    {onnx_size:.2f} MB")
    print(f"  èŠ‚çœ:    {pytorch_size - onnx_size:.2f} MB ({(1 - onnx_size/pytorch_size)*100:.1f}%)")

    print(f"\n{'='*70}")
    print(f"  âœ… è½¬æ¢å®Œæˆ!")
    print(f"{'='*70}\n")

    print(f"ä½¿ç”¨æ–¹æ³•:")
    print(f"  1. å°† {onnx_model_path} ä¸Šä¼ åˆ°ECSæœåŠ¡å™¨")
    print(f"  2. å°† {meta_path} ä¸Šä¼ åˆ°ECSæœåŠ¡å™¨")
    print(f"  3. ä½¿ç”¨ requirements_onnx.txt å®‰è£…ä¾èµ–")
    print(f"  4. è¿è¡Œ paper_trader.py æ—¶ä¼šè‡ªåŠ¨ä½¿ç”¨ONNXæ¨¡å‹")

    return True


def main():
    import argparse

    parser = argparse.ArgumentParser(description="PyTorchæ¨¡å‹è½¬ONNX")
    parser.add_argument(
        "--pytorch-model",
        type=Path,
        default=Path("models/rank_model.pt"),
        help="PyTorchæ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--onnx-model",
        type=Path,
        default=Path("models/rank_model.onnx"),
        help="è¾“å‡ºONNXæ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--opset-version",
        type=int,
        default=14,
        help="ONNX opsetç‰ˆæœ¬"
    )

    args = parser.parse_args()

    success = convert_to_onnx(
        pytorch_model_path=args.pytorch_model,
        onnx_model_path=args.onnx_model,
        opset_version=args.opset_version
    )

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
